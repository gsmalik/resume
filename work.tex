Total work experience of 5+ years. List highlights recent, relevant experience and is not exhaustive.
% abr
\datedsubsection{Sep. 2019 -- Present}
    {
	    % company and place
    	Applied Brain Research (Waterloo, ON, Canada)}
    {
	    % title
    	$\sbullet$ Machine Learning Scientist III
    }
    {
        % job highlights
        \justify
        \begin{itemize}
            % tinyml work
            \item [--] Developed SotA keyword spotting recurrent neural networks that achieves 95.9-96.9\% classification accuracy on SpeechCommands while consuming 2-10$\times$ fewer parameters and 16-24$\times$ less power compared to previous SotA. Critically, these neural networks are stateful, that is, the internal state of the network is not reset between samples and hence they can be deployed in the real world.
            % patent work
           \item  [--] Developed methods and systems for efficient processing of RNNs, using which an end to end RNN-T network was implemented for full scale ASR that consumes only 4-8$\mu$Ws and can infer samples in real time, that is, has an end to end latency which is less than the length of an audio window.
           % cloud work
           \item [--] Developed a web based deep learning framework for deploying ASR systems at the edge. The cloud system creates a hardware agnostic environment where a user can create, train and test their networks for a wide variety of edge AI platforms ($\mu$Cs, TPUs etc) without writing a single line of code. Users can also upload their own datasets, record their own audio live to test streaming performance of networks and get a rich cache of information like confusion matrices, power consumption etc.
	   \end{itemize}
	}
    
% phd ai reserach intern
\datedsubsection{Jan. 2019 -- Apr. 2019}
    {
	    % company and place
	    Xilinx (Dublin, Ireland)}
    {
	    % title
	    $\sbullet$ AI Research Intern (PhD Research Term)
    }
    {
	    % job highlights
	    \justify
        \begin{itemize}
            \item [--]  Created \textit{DarwiNN}, an open-source, GPU-accelerated distributed toolbox based on pyTorch for training deep neural networks using evolutionary strategies.
            \item [--]  Developed \textit{Chromosome Updates}, a communication optimized method for distributing training in \textit{DarwiNN}, that reduces communication across learning agents by a factor of 2$\times$ and is an exact calculation of network gradients.
	   \end{itemize}
    }
    
% reniac
\datedsubsection{Feb. 2016 -- Jul. 2017}
    {
        % company and place
	    Reniac (Hyderabad, India)}
    {
        % title
	    $\sbullet$ Systems Design Engineer
    }
    {
        % job highlights
        \justify
        \begin{itemize}
            \item [--] Developed a data base proxy engine to accelerate and optimise existing open source database implementations for a variety of data heavy applications that can be implemented as an FPGA optimised cache, a transparent proxy or a storage engine. 
            \item [--] The engine delivers 36$\times$ lower latency, 20$\times$ increased throughput and 10$\times$ increase in data capacity for consistent SLAs.
        \end{itemize}
    }
% collabera
% \datedsubsection{Aug. 2015 -- Feb. 2016}
%     {
%         % company and place
% 	    Collabera (Hyderabad, India)}
%     {
%         % title
% 	    $\sbullet$ Software Engineer
%     }
%     {
%         % job highlights
%         \begin{itemize}
%             \item [--] i did this
%             \item [--] i did that
%         \end{itemize}
%     }

% \subsection{Internship roles}